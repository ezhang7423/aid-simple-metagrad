{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchviz\n",
      "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/ezipe/.miniconda3/envs/py311-torch/lib/python3.11/site-packages (from torchviz) (2.1.1)\n",
      "Collecting graphviz (from torchviz)\n",
      "  Obtaining dependency information for graphviz from https://files.pythonhosted.org/packages/00/be/d59db2d1d52697c6adc9eacaf50e8965b6345cc143f671e1ed068818d5cf/graphviz-0.20.3-py3-none-any.whl.metadata\n",
      "  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /home/ezipe/.miniconda3/envs/py311-torch/lib/python3.11/site-packages (from torch->torchviz) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions in /home/ezipe/.miniconda3/envs/py311-torch/lib/python3.11/site-packages (from torch->torchviz) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/ezipe/.miniconda3/envs/py311-torch/lib/python3.11/site-packages (from torch->torchviz) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ezipe/.miniconda3/envs/py311-torch/lib/python3.11/site-packages (from torch->torchviz) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ezipe/.miniconda3/envs/py311-torch/lib/python3.11/site-packages (from torch->torchviz) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/ezipe/.miniconda3/envs/py311-torch/lib/python3.11/site-packages (from torch->torchviz) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ezipe/.miniconda3/envs/py311-torch/lib/python3.11/site-packages (from torch->torchviz) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ezipe/.miniconda3/envs/py311-torch/lib/python3.11/site-packages (from torch->torchviz) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ezipe/.miniconda3/envs/py311-torch/lib/python3.11/site-packages (from torch->torchviz) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ezipe/.miniconda3/envs/py311-torch/lib/python3.11/site-packages (from torch->torchviz) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ezipe/.miniconda3/envs/py311-torch/lib/python3.11/site-packages (from torch->torchviz) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ezipe/.miniconda3/envs/py311-torch/lib/python3.11/site-packages (from torch->torchviz) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ezipe/.miniconda3/envs/py311-torch/lib/python3.11/site-packages (from torch->torchviz) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ezipe/.miniconda3/envs/py311-torch/lib/python3.11/site-packages (from torch->torchviz) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ezipe/.miniconda3/envs/py311-torch/lib/python3.11/site-packages (from torch->torchviz) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/ezipe/.miniconda3/envs/py311-torch/lib/python3.11/site-packages (from torch->torchviz) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ezipe/.miniconda3/envs/py311-torch/lib/python3.11/site-packages (from torch->torchviz) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/ezipe/.miniconda3/envs/py311-torch/lib/python3.11/site-packages (from torch->torchviz) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ezipe/.miniconda3/envs/py311-torch/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->torchviz) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ezipe/.miniconda3/envs/py311-torch/lib/python3.11/site-packages (from jinja2->torch->torchviz) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ezipe/.miniconda3/envs/py311-torch/lib/python3.11/site-packages (from sympy->torch->torchviz) (1.3.0)\n",
      "Downloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m813.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: torchviz\n",
      "  Building wheel for torchviz (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4132 sha256=6c85894deedb7af82edfde7049f143ae35c924a78a8a2b35551be607c17e9df3\n",
      "  Stored in directory: /home/ezipe/.cache/pip/wheels/5a/d0/3f/b7014553eb74f12892b7d9b69c6083044564712d10fde8dfdc\n",
      "Successfully built torchviz\n",
      "Installing collected packages: graphviz, torchviz\n",
      "Successfully installed graphviz-0.20.3 torchviz-0.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15.1543, 15.1543], grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_442945/3416601274.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  theta = torch.tensor(torch.ones(2), requires_grad=True)\n",
      "/tmp/ipykernel_442945/3416601274.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  eta = torch.tensor(torch.ones((3)), requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pg_with_exp.png'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "lr = 100.\n",
    "\n",
    "# initialization\n",
    "theta = torch.tensor(torch.ones(2), requires_grad=True)\n",
    "theta_prime = theta.clone().detach()\n",
    "theta_prime.requires_grad = True\n",
    "\n",
    "eta = torch.tensor(torch.ones((3)), requires_grad=True)\n",
    "# eta = torch.nn.Parameter(torch.tensor([1.], requires_grad=True))\n",
    "\n",
    "# update theta_prime w.r.t some pg loss\n",
    "pg = (eta.sum() * theta_prime).sum()\n",
    "pg.backward()\n",
    "theta_prime = theta_prime + lr * theta_prime.grad \n",
    "eta.grad.zero_() # remove the gradient from eta\n",
    "eta.requires_grad = True\n",
    "\n",
    "# theta.grad is nothing here because we called detach\n",
    "pg = (eta.mean() * theta).sum().exp()\n",
    "\n",
    "dl_dtheta = torch.autograd.grad(pg, theta, create_graph=True)[0]\n",
    "theta_hat = theta + lr * dl_dtheta\n",
    "\n",
    "# # calculate the meta loss\n",
    "incentive_loss = theta_hat.sum()\n",
    "cost_loss = eta.sum() * 10\n",
    "loss = incentive_loss + cost_loss\n",
    "loss.backward()\n",
    "eta = eta + lr * eta.grad\n",
    "\n",
    "# # sync theta to theta_hat\n",
    "theta = theta_hat.clone().detach()\n",
    "dot = make_dot(loss, params=dict(eta=eta, theta=theta, theta_prime=theta_prime, loss=loss))\n",
    "\n",
    "dot.render(\"pg_with_exp\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_442945/1917990666.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  theta = torch.tensor(torch.ones(2), requires_grad=True)\n",
      "/tmp/ipykernel_442945/1917990666.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  eta = torch.tensor(torch.ones((3)), requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pg_without_exp.png'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "lr = 100.\n",
    "\n",
    "# initialization\n",
    "theta = torch.tensor(torch.ones(2), requires_grad=True)\n",
    "theta_prime = theta.clone().detach()\n",
    "theta_prime.requires_grad = True\n",
    "\n",
    "eta = torch.tensor(torch.ones((3)), requires_grad=True)\n",
    "# eta = torch.nn.Parameter(torch.tensor([1.], requires_grad=True))\n",
    "\n",
    "# update theta_prime w.r.t some pg loss\n",
    "pg = (eta.sum() * theta_prime).sum()\n",
    "pg.backward()\n",
    "theta_prime = theta_prime + lr * theta_prime.grad \n",
    "eta.grad.zero_() # remove the gradient from eta\n",
    "eta.requires_grad = True\n",
    "\n",
    "# theta.grad is nothing here because we called detach\n",
    "pg = (eta.mean() * theta).sum()\n",
    "\n",
    "dl_dtheta = torch.autograd.grad(pg, theta, create_graph=True)[0]\n",
    "theta_hat = theta + lr * dl_dtheta\n",
    "\n",
    "# # calculate the meta loss\n",
    "incentive_loss = theta_hat.sum()\n",
    "cost_loss = eta.sum() * 10\n",
    "loss = incentive_loss + cost_loss\n",
    "loss.backward()\n",
    "eta = eta + lr * eta.grad\n",
    "\n",
    "# # sync theta to theta_hat\n",
    "theta = theta_hat.clone().detach()\n",
    "dot = make_dot(loss, params=dict(eta=eta, theta=theta, theta_prime=theta_prime, loss=loss))\n",
    "dot.render(\"pg_without_exp\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_442945/873365187.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  eta.grad\n"
     ]
    }
   ],
   "source": [
    "eta.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"228pt\" height=\"215pt\"\n",
       " viewBox=\"0.00 0.00 228.00 215.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 211)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-211 224,-211 224,4 -4,4\"/>\n",
       "<!-- 125202014551120 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>125202014551120</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"142,-31 77,-31 77,0 142,0 142,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"109.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (1, 10)</text>\n",
       "</g>\n",
       "<!-- 125202014656240 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>125202014656240</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"154,-86 65,-86 65,-67 154,-67 154,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"109.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 125202014656240&#45;&gt;125202014551120 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>125202014656240&#45;&gt;125202014551120</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M109.5,-66.79C109.5,-60.07 109.5,-50.4 109.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"113,-41.19 109.5,-31.19 106,-41.19 113,-41.19\"/>\n",
       "</g>\n",
       "<!-- 125202014656816 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>125202014656816</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-141 0,-141 0,-122 101,-122 101,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 125202014656816&#45;&gt;125202014656240 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>125202014656816&#45;&gt;125202014656240</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M59.98,-121.98C68.7,-114.15 81.84,-102.34 92.4,-92.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"94.75,-95.46 99.85,-86.17 90.07,-90.25 94.75,-95.46\"/>\n",
       "</g>\n",
       "<!-- 125202014551216 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>125202014551216</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"83,-207 18,-207 18,-177 83,-177 83,-207\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-195\" font-family=\"monospace\" font-size=\"10.00\">x</text>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\"> (1, 10)</text>\n",
       "</g>\n",
       "<!-- 125202014551216&#45;&gt;125202014656816 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>125202014551216&#45;&gt;125202014656816</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-176.84C50.5,-169.21 50.5,-159.7 50.5,-151.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-151.27 50.5,-141.27 47,-151.27 54,-151.27\"/>\n",
       "</g>\n",
       "<!-- 125202014657440 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>125202014657440</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"220,-141 119,-141 119,-122 220,-122 220,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"169.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 125202014657440&#45;&gt;125202014656240 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>125202014657440&#45;&gt;125202014656240</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M159.86,-121.98C150.99,-114.15 137.63,-102.34 126.89,-92.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"129.13,-90.17 119.32,-86.17 124.49,-95.41 129.13,-90.17\"/>\n",
       "</g>\n",
       "<!-- 125202014551024 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>125202014551024</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"202,-207 137,-207 137,-177 202,-177 202,-207\"/>\n",
       "<text text-anchor=\"middle\" x=\"169.5\" y=\"-195\" font-family=\"monospace\" font-size=\"10.00\">y</text>\n",
       "<text text-anchor=\"middle\" x=\"169.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\"> (1, 10)</text>\n",
       "</g>\n",
       "<!-- 125202014551024&#45;&gt;125202014657440 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>125202014551024&#45;&gt;125202014657440</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M169.5,-176.84C169.5,-169.21 169.5,-159.7 169.5,-151.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"173,-151.27 169.5,-141.27 166,-151.27 173,-151.27\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x71dedd9bbe90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "\n",
    "x = torch.randn(1, 10, requires_grad=True)\n",
    "\n",
    "y = torch.randn(1, 10, requires_grad=True)\n",
    "\n",
    "z = x + y\n",
    "\n",
    "make_dot(z, params={\"x\": x, \"y\": y})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
